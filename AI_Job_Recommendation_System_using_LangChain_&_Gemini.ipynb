{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGzL7i7Xv0jA"
      },
      "source": [
        "####**1. Installing and importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq2l8KyIPJHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca72ed4-e4e0-4971-9773-8535521c7520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: requests==2.32.4 in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (2025.10.5)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.78)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (0.6.15)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (2.25.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15) (5.29.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Downloading langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.8.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.7.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (4.9.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.184.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.35.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage==0.6.15) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.5)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=341a29113aebfddbd620a50340a2556f8d62bed804d048b1780c0df1ce63fda5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, filetype, durationpy, uvloop, urllib3, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, pydeck, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, streamlit, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-google-genai\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "Successfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.1 coloredlogs-15.0.1 durationpy-0.10 filetype-1.2.0 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 langchain-google-genai-2.0.10 mmh3-5.2.0 onnxruntime-1.23.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pydeck-0.9.1 pypika-0.48.9 streamlit-1.50.0 urllib3-2.3.0 uvloop-0.21.0 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community requests==2.32.4\n",
        "!pip install langchain streamlit chromadb langchain-google-genai google-ai-generativelanguage==0.6.15\n",
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L11-KksVvPM"
      },
      "source": [
        "####**2. Setting Google Gemini API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B6n4D4Jj2tU",
        "outputId": "c2d94e9f-8970-418f-ab2c-1b65174a4ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "pip install -U google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gmFBrEtkPTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2cdae955-bba8-4a2d-986b-b3f6cab7858d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure with your Gemini API key\n",
        "genai.configure(api_key=\"AIzaSyA4s4XgRUhxx5so1XJ0P2QSUa5mTAmbOCE\")\n",
        "\n",
        "# Load the correct model\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Generate content\n",
        "response = model.generate_content(\"Hello Gemini.\")\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsF12Ca39XNX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA4s4XgRUhxx5so1XJ0P2QSUa5mTAmbOCE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MbJ4HvxWDak"
      },
      "source": [
        "####**3. Scraping LinkedIn job website using Requests and BeautifulSoup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS3yftAHaQ7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94b95bf-9275-4c00-e0a5-9920a0215665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTs5NEJLB5i9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9170ab8-60e7-41f7-a082-71681348ac68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Data Analyst', 'company': 'Curefit', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Experian', 'location': 'Hyderabad, Telangana, India'}\n",
            "{'title': 'Data Analyst', 'company': 'V-Mart Retail Ltd.', 'location': 'Gurugram, Haryana, India'}\n",
            "{'title': 'Data Analyst, Risk', 'company': 'Google', 'location': 'Hyderabad, Telangana, India'}\n",
            "{'title': 'Data Analyst - LuLu Omnichannel', 'company': 'Lulu Retail', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'NxtWave', 'location': 'Hyderabad, Telangana, India'}\n",
            "{'title': 'Reporting & Dashboard Analyst', 'company': 'OGS', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Reporting & Dashboard Analyst', 'company': 'OGS', 'location': 'Hyderabad, Telangana, India'}\n",
            "{'title': 'Data Analyst : Immediate Joiner', 'company': 'Rackspace Technology', 'location': 'Gurgaon, Haryana, India'}\n",
            "{'title': 'Data Analyst [T500-20905]', 'company': 'Delta Air Lines', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst Intern', 'company': 'Exotel', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Healthcare Data Analyst', 'company': 'Infosys', 'location': 'Bengaluru East, Karnataka, India'}\n",
            "{'title': 'Reporting & Dashboard Analyst', 'company': 'OGS', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'GoodScore', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Insights and Visualization', 'company': 'Infosys', 'location': 'Bengaluru East, Karnataka, India'}\n",
            "{'title': 'Data Analyst (SQL,HDFS, Hive)', 'company': 'RiskInsight Consulting Pvt Ltd.', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'DATA ANALYST- Power BI & SQL', 'company': 'IBM', 'location': 'Bengaluru East, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Unilever', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst [T500-19572]', 'company': 'MUFG', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst [T500-20653]', 'company': 'Talent500', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst, EMSX', 'company': 'Amazon', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Reporting Analyst', 'company': 'Korn Ferry', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analytics (SQL Query + Data Visualization)', 'company': 'Tata Consultancy Services', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Recro', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'LSEG', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'DIGITAL HARBOR, Inc.', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Tata Consultancy Services', 'location': 'Greater Bengaluru Area'}\n",
            "{'title': 'Analytics Associate', 'company': 'smallcase', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Intel Corporation', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Analyst', 'company': 'slice', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Analyst', 'company': 'slice', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Get Well', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst - Power BI', 'company': 'Gainwell Technologies', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'BNP Paribas', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'S&P Global', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'JBPCO India', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Infogain', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst - L4', 'company': 'Wipro', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analytics & Reporting - Analyst', 'company': 'JPMorganChase', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Randstad', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Analyst, Trust and Safety', 'company': 'Google', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Solventum', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Anlage Infotech (India) P Ltd', 'location': 'Bangalore Rural, Karnataka, India'}\n",
            "{'title': 'Business Analyst – BI', 'company': 'Calpion Inc.', 'location': 'Bengaluru East, Karnataka, India'}\n",
            "{'title': 'Associate', 'company': 'PwC India', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data Analyst', 'company': 'Deloitte', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Data reporting - Associate 2', 'company': 'PwC Acceleration Center India', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Walk-in For Reference Data at Bangalore on 8th October 25', 'company': 'Infosys BPM', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Client Data Analyst', 'company': 'JPMorganChase', 'location': 'Bengaluru, Karnataka, India'}\n",
            "{'title': 'Client Data Analyst', 'company': 'JPMorganChase', 'location': 'Bengaluru, Karnataka, India'}\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def linkedinjobs(search, location, num_pages):\n",
        "    all_jobs = []  # Use a different list to store the parsed job data\n",
        "    for page in range(num_pages):\n",
        "        url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={search}&location={location}&start={page * 10}\"\n",
        "        html = requests.get(url).text\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        jobs_on_page = soup.find_all('li') # This is the BeautifulSoup ResultSet\n",
        "\n",
        "        for job in jobs_on_page:  # Iterate over the BeautifulSoup objects\n",
        "            info = job.select_one('.base-search-card__info')\n",
        "            if not info:\n",
        "                continue\n",
        "\n",
        "            title = info.select_one('.base-search-card__title')\n",
        "            company = info.select_one('.base-search-card__subtitle')\n",
        "            location = info.select_one('.job-search-card__location')\n",
        "\n",
        "            all_jobs.append({\n",
        "                \"title\": title.get_text(strip=True) if title else \"\",\n",
        "                \"company\": company.get_text(strip=True) if company else \"\",\n",
        "                \"location\": location.get_text(strip=True) if location else \"\"\n",
        "            })\n",
        "    return all_jobs # Return the list of dictionaries\n",
        "\n",
        "jobs_list = linkedinjobs(\"data analyst\", \"India\", 5) # Call the function and store the result in a new variable # Inspect the result\n",
        "for jobs in jobs_list[:50]:\n",
        "  print(jobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ1bHhNwWWjO"
      },
      "source": [
        "####**4. Extracting the jobs data in JSON format**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{'title': 'Data Analyst', 'company': 'Curefit', 'location': 'Bengaluru, Karnataka, India'} -> [0.2,5.5,6.7,0.4,2.5] -> Embedding\n",
        "Entire data conversion in vctor form is called as Vector Embeddings"
      ],
      "metadata": {
        "id": "CzB3jGUfoRW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLK2EF7YTzOO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96ce0bc9-f880-4625-df95-5169106d5bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'linkedin_jobs.json' has been saved in the '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation/Data' folder.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_259f3cb6-4913-4b4a-bceb-23abe3e68a01\", \"linkedin_jobs.json\", 6932)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Assuming jobs_list is available from previous cell execution\n",
        "\n",
        "# Define the folder name and filename\n",
        "folder_name = '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation/Data'\n",
        "filename = 'linkedin_jobs.json'\n",
        "filepath = os.path.join(folder_name, filename)\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Save the jobs_list to a JSON file inside the folder\n",
        "with open(filepath, 'w') as f:\n",
        "    json.dump(jobs_list, f, indent=4)\n",
        "\n",
        "print(f\"'{filename}' has been saved in the '{folder_name}' folder.\")\n",
        "\n",
        "# Make the file downloadable\n",
        "files.download(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9Uj0B8rQO03"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-chroma chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4BgxzPWkGD"
      },
      "source": [
        "####**5. Storing the job data in Vector Embeddings in Chroma Database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1htbLgbwYar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebb0d0c6-56a1-4cd2-821a-0586fc7fbe00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-preview\n",
            "models/veo-3.0-fast-generate-preview\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knhgsL-3aUCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ab2bd3-e22a-4681-a147-d9360a11a0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store and retriever initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure with your Gemini API key\n",
        "genai.configure(api_key=\"AIzaSyA4s4XgRUhxx5so1XJ0P2QSUa5mTAmbOCE\")\n",
        "\n",
        "\n",
        "# Prepare documents\n",
        "# Assuming 'jobs_list' is already populated from the previous cell's execution\n",
        "documents = [Document(page_content=f\"{job['title']} at {job['company']} in {job['location']}\", metadata=job) for job in jobs_list]\n",
        "\n",
        "# Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=90, chunk_overlap=10)\n",
        "chunk_docs = []\n",
        "for doc in documents:\n",
        "    for chunk in splitter.split_text(doc.page_content):\n",
        "        chunk_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n",
        "# Embeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=\"AIzaSyA4s4XgRUhxx5so1XJ0P2QSUa5mTAmbOCE\")\n",
        "\n",
        "# Chroma vector store\n",
        "vectordb = Chroma.from_documents(documents=chunk_docs, embedding=embeddings)\n",
        "\n",
        "# Create a retriever\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "print(\"Vector store and retriever initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecdfe94a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1354bdc-ed62-4baf-86a7-fc68bdd87c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store and retriever initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "def initialize_vector_store(jobs_list, chunk_size=100, chunk_overlap=50):\n",
        "    \"\"\"Initializes a Chroma vector store and retriever with job data.\n",
        "\n",
        "    Args:\n",
        "        jobs_list: A list of dictionaries containing job information.\n",
        "        chunk_size: The size of the text chunks.\n",
        "        chunk_overlap: The overlap between text chunks.\n",
        "\n",
        "    Returns:\n",
        "        A Chroma retriever initialized with the job data.\n",
        "    \"\"\"\n",
        "    # Prepare documents\n",
        "    documents = [Document(page_content=f\"{job['title']} at {job['company']} in {job['location']}\", metadata=job) for job in jobs_list]\n",
        "\n",
        "    # Split into chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunk_docs = []\n",
        "    for doc in documents:\n",
        "        for chunk in splitter.split_text(doc.page_content):\n",
        "            chunk_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "    # Chroma vector store\n",
        "    vectordb = Chroma.from_documents(documents=chunk_docs, embedding=embeddings)\n",
        "\n",
        "    # Create a retriever\n",
        "    retriever = vectordb.as_retriever()\n",
        "\n",
        "    print(\"Vector store and retriever initialized successfully!\")\n",
        "    return retriever\n",
        "\n",
        "\n",
        "# Example usage (assuming jobs_list is available from previous cell execution)\n",
        "retriever = initialize_vector_store(jobs_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbPs90AcXp8j"
      },
      "source": [
        "####**6. Turning Chroma database into a Retriever, which searches for most relevant jobs based on the given prompt using Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vUwc8dfBYqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27fc04ab-74bf-4480-fb46-dd2f5b427707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store and retriever initialized successfully!\n",
            "Some data analyst jobs in Bangalore include Data Analyst [T500-20653] at Talent500 and Data Analyst - L4 at Wipro. Both positions are located in Bengaluru, Karnataka, India.\n",
            "Thanks for asking!\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "# Assuming 'jobs_list' is available from previous cell execution\n",
        "# Initialize the retriever using the function\n",
        "retriever = initialize_vector_store(jobs_list)\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"Thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "# Initialize the Google Generative AI model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# Create the Retrieval QA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "\n",
        "# Run the chain with a sample query\n",
        "query = \"What are some data analyst jobs in Bangalore?\"\n",
        "result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-xkRIW7P-vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e7ff9f1-2a43-448a-e092-57bf5c99eeb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended Jobs:\n",
            " Here are some job recommendations based on your skills:\n",
            "\n",
            "*   **Data Analyst (SQL,HDFS, Hive)** at RiskInsight Consulting Pvt Ltd. in Bengaluru, Karnataka, India\n",
            "*   **Data Analyst** at Randstad in Bengaluru, Karnataka, India\n",
            "\n",
            "Source 1:\n",
            "Data Analyst (SQL,HDFS, Hive) at RiskInsight Consulting Pvt Ltd. in Bengaluru, Karnataka,\n",
            "\n",
            "Source 2:\n",
            "Data Analyst at Randstad in Bengaluru, Karnataka, India\n",
            "\n",
            "Source 3:\n",
            "Data Analyst at Randstad in Bengaluru, Karnataka, India\n",
            "\n",
            "Source 4:\n",
            "Data Analyst at Randstad in Bengaluru, Karnataka, India\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "def get_job_recommendations_with_sources(retriever, llm, prompt, user_profile):\n",
        "    \"\"\"\n",
        "    Sets up and runs a RAG chain to get job recommendations and source documents.\n",
        "\n",
        "    Args:\n",
        "        retriever: The initialized vector store retriever.\n",
        "        llm: The initialized language model.\n",
        "        prompt: The prompt template.\n",
        "        user_profile: The user's skills and preferences string.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the recommended jobs (answer) and source documents.\n",
        "    \"\"\"\n",
        "    # Build the RAG chain using LCEL to return both answer and sources\n",
        "    rag_chain_with_sources = RunnableParallel(\n",
        "        answer= (\n",
        "            {\"context\": retriever, \"query\": RunnablePassthrough()}\n",
        "            | prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        ),\n",
        "        sources= RunnablePassthrough() | retriever # Directly pass the original query to the retriever\n",
        "    )\n",
        "\n",
        "    response_with_sources = rag_chain_with_sources.invoke(user_profile)\n",
        "    return response_with_sources\n",
        "\n",
        "# Assuming retriever, llm, and prompt are initialized from previous cells\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = \"\"\"\n",
        "You are an AI job recommendation assistant.\n",
        "Given the user's skills and preferences, recommend up to four different and most relevant job openings from the retrieved postings.\n",
        "Ensure the recommendations are distinct job titles.\n",
        "\n",
        "User profile:\n",
        "{query}\n",
        "\n",
        "Relevant jobs:\n",
        "{context}\n",
        "\n",
        "Provide recommendations in bullet points with job title, company, and location. If you cannot find four distinct jobs, provide as many distinct ones as possible.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Example usage\n",
        "user_profile = \"Skills: Python, Data Analysis, Machine Learning..\"\n",
        "recommendations = get_job_recommendations_with_sources(retriever, llm, prompt, user_profile)\n",
        "print(\"Recommended Jobs:\\n\", recommendations[\"answer\"])\n",
        "for i, doc in enumerate(recommendations[\"sources\"], start=1):\n",
        "    print(f\"\\nSource {i}:\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh9FTWSFYeFC"
      },
      "source": [
        "####**7. Creating a streamlit App to Run the Application in the browser**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjA0muk0z4fD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "import tempfile\n",
        "from langchain.schema import Document # Import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Helper function to initialize vector store (adapted for Streamlit in Colab) ---\n",
        "@st.cache_resource\n",
        "def initialize_vector_store_for_streamlit(job_data):\n",
        "    \"\"\"Initializes a Chroma vector store and retriever with job data.\"\"\"\n",
        "    if not job_data:\n",
        "        st.error(\"Job data is empty. Please ensure the data loading cell was run successfully.\")\n",
        "        return None\n",
        "\n",
        "    documents = [Document(page_content=f\"{job['title']} at {job['company']} in {job['location']}\", metadata=job) for job in job_data]\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunk_docs = []\n",
        "    for doc in documents:\n",
        "        for chunk in splitter.split_text(doc.page_content):\n",
        "            chunk_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n",
        "    google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if not google_api_key:\n",
        "        st.error(\"Google API Key not found in Colab secrets. Please add it as 'GOOGLE_API_KEY'.\")\n",
        "        return None\n",
        "\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=google_api_key)\n",
        "\n",
        "    vectordb = Chroma.from_documents(documents=chunk_docs, embedding=embeddings)\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "\n",
        "    st.success(\"Vector store and retriever initialized successfully!\")\n",
        "    return retriever\n",
        "\n",
        "# Store the streamlit app code in a variable\n",
        "streamlit_code = \"\"\"\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "import tempfile\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# --- Helper function to initialize vector store (adapted for Streamlit in Colab) ---\n",
        "@st.cache_resource\n",
        "def initialize_vector_store_for_streamlit(job_data):\n",
        "    \\\"\\\"\\\"Initializes a Chroma vector store and retriever with job data.\\\"\\\"\\\"\n",
        "    if not job_data:\n",
        "        st.error(\"Job data is empty. Please ensure the data loading cell was run successfully.\")\n",
        "        return None\n",
        "\n",
        "    documents = [Document(page_content=f\"{job['title']} at {job['company']} in {job['location']}\", metadata=job) for job in job_data]\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunk_docs = []\n",
        "    for doc in documents:\n",
        "        for chunk in splitter.split_text(doc.page_content):\n",
        "            chunk_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n",
        "    google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if not google_api_key:\n",
        "        st.error(\"Google API Key not found in Colab secrets. Please add it as 'GOOGLE_API_KEY'.\")\n",
        "        return None\n",
        "\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=google_api_key)\n",
        "\n",
        "    vectordb = Chroma.from_documents(documents=chunk_docs, embedding=embeddings)\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "\n",
        "    st.success(\"Vector store and retriever initialized successfully!\")\n",
        "    return retriever\n",
        "\n",
        "# --- Load and Initialize ---\n",
        "# Assuming jobs_list is available from previous cell execution (Note: In a real script, you'd load this from a file)\n",
        "# For this example in Colab, we'll assume jobs_list is globally available from previous cells\n",
        "# In a standalone app.py, you would load your data here.\n",
        "try:\n",
        "    # Access jobs_list from the global scope if running in Colab notebook\n",
        "    # In a standalone script, you would load your data from a file here.\n",
        "    # For demonstration in Colab, we assume jobs_list is available.\n",
        "    # Replace with your data loading logic if running as a standalone script.\n",
        "    global jobs_list\n",
        "    if 'jobs_list' not in globals():\n",
        "        st.error(\"Job data (jobs_list) is not available. Please ensure the data loading cell was run successfully in the notebook before saving and running this app.\")\n",
        "        jobs_list = [] # Initialize as empty to prevent further errors\n",
        "\n",
        "    retriever = initialize_vector_store_for_streamlit(jobs_list)\n",
        "except NameError:\n",
        "    st.error(\"Job data (jobs_list) is not available. Please run the data loading and vector store initialization cells first in the notebook.\")\n",
        "    retriever = None\n",
        "\n",
        "\n",
        "if retriever:\n",
        "    google_api_key = userdata.get('GOOGLE_API_KEY') # Access secrets in Colab\n",
        "    if google_api_key:\n",
        "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0, google_api_key=google_api_key)\n",
        "    else:\n",
        "        st.error(\"Google API Key not found for LLM. Please add it as 'GOOGLE_API_KEY' in Colab secrets.\")\n",
        "        llm = None\n",
        "\n",
        "    if llm:\n",
        "        prompt_template = \\\"\\\"\\\"\n",
        "        You are an AI job recommendation assistant.\n",
        "        Given the user's skills and preferences, recommend the most relevant job openings from the retrieved postings.\n",
        "\n",
        "        User profile:\n",
        "        {query}\n",
        "\n",
        "        Relevant jobs:\n",
        "        {context}\n",
        "\n",
        "        Provide recommendations in bullet points with job title, company, and location.\n",
        "        \\\"\\\"\\\"\n",
        "        prompt = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"query\", \"context\"]\n",
        "        )\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            chain_type=\"stuff\",\n",
        "            chain_type_kwargs={\"prompt\": prompt},\n",
        "            return_source_documents=True,\n",
        "            input_key=\"query\"\n",
        "        )\n",
        "\n",
        "        # Streamlit UI\n",
        "        st.title(\"💼 AI Job Recommendation System\")\n",
        "        st.write(\"Discover jobs that fit your skills & preferences.\")\n",
        "\n",
        "        user_input = st.text_area(\"Enter your skills, experience, and job preferences:\")\n",
        "        # Upload functionality is not fully implemented for resume parsing in this example\n",
        "        uploaded_file = st.file_uploader(\"Upload your resume (Optional):\", type=[\"pdf\", \"docx\", \"txt\"])\n",
        "\n",
        "        resume_content = \"\"\n",
        "        if uploaded_file is not None:\n",
        "            # In a real application, you would add resume parsing logic here\n",
        "            st.info(f\"Resume '{uploaded_file.name}' uploaded. (Resume parsing functionality not implemented in this example)\")\n",
        "            # Example of how you might read a text file:\n",
        "            # if uploaded_file.type == \"text/plain\":\n",
        "            #     resume_content = uploaded_file.getvalue().decode(\"utf-8\")\n",
        "\n",
        "\n",
        "        if st.button(\"Get Recommendations\"):\n",
        "            if user_input.strip() or uploaded_file is not None:\n",
        "                query = user_input.strip()\n",
        "                # If only a resume is uploaded, you might process it and use its content as the query\n",
        "                # For now, we'll just use the user_input text.\n",
        "                # If you implement resume parsing, you would combine or replace query with resume_content\n",
        "                if uploaded_file is not None and not query:\n",
        "                     # This part would be more sophisticated with actual resume parsing\n",
        "                     st.warning(\"Resume uploaded but no text input. Resume parsing not implemented.\")\n",
        "                     query = \"Recommend jobs based on general data analysis skills.\" # Fallback or process resume content\n",
        "\n",
        "                if not query:\n",
        "                    st.warning(\"Please enter your profile information or ensure resume processing is added.\")\n",
        "                else:\n",
        "                    with st.spinner(\"Finding best matches...\"):\n",
        "                        response = qa_chain.invoke({\"query\": query})\n",
        "                    st.subheader(\"Recommended Jobs\")\n",
        "                    st.write(response[\"result\"])\n",
        "\n",
        "                    with st.expander(\"See Source Job Descriptions\"):\n",
        "                        if response[\"source_documents\"]:\n",
        "                            for i, doc in enumerate(response[\"source_documents\"], start=1):\n",
        "                                st.markdown(f\"**Source {i}:**\")\n",
        "                                st.write(doc.page_content)\n",
        "                        else:\n",
        "                            st.info(\"No source documents found for this query.\")\n",
        "            else:\n",
        "                st.warning(\"Please enter your profile information or upload a resume.\")\n",
        "\n",
        "    else:\n",
        "        st.error(\"LLM could not be initialized. Please check your Google API Key.\")\n",
        "else:\n",
        "    st.error(\"Retriever could not be initialized. Please check the job data and API key.\")\n",
        "\"\"\"\n",
        "\n",
        "# The rest of the code in this cell (if any) would be executed after streamlit_code is defined.\n",
        "# However, since the Streamlit app should be saved to a file and run separately,\n",
        "# there's no need to execute the app code directly here.\n",
        "# The primary purpose of THIS cell is now just to define the streamlit_code string.\n",
        "\n",
        "# Any subsequent cells that use streamlit_code will now have access to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefMHEWsZD09"
      },
      "source": [
        "####**8. Saving the Streamlit app.py file in the folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO0TxqqnSHzL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58674403-6c3f-4345-cbf7-413674ccbf01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'app.py' has been saved in the '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation' folder.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3781ac83-be0e-47c7-bbab-72b05d719a26\", \"app.py\", 6834)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming streamlit_code is available from cell VLF3CKHwssbp\n",
        "\n",
        "# Define the folder name and filename\n",
        "folder_name = '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation'\n",
        "filename = 'app.py'\n",
        "filepath = os.path.join(folder_name, filename)\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Save the streamlit_code to a Python file inside the folder\n",
        "try:\n",
        "    with open(filepath, 'w') as f:\n",
        "        # Access the streamlit_code variable from the notebook's global scope\n",
        "        f.write(streamlit_code)\n",
        "    print(f\"'{filename}' has been saved in the '{folder_name}' folder.\")\n",
        "\n",
        "    # Make the file downloadable\n",
        "    files.download(filepath)\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: 'streamlit_code' variable not found. Please ensure the cell defining the Streamlit code (cell VLF3CKHwssbp) has been executed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2VyvMCKZhCM"
      },
      "source": [
        "####**9. Creating and Saving the Requirements.txt file in the folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h7hm70e0Ldd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edeb01cb-1b8c-45a5-e846-15451de47723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'requirements.txt' has been saved in the '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation' folder.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the folder name and filename\n",
        "folder_name = '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation'\n",
        "filename = 'requirements.txt'\n",
        "filepath = os.path.join(folder_name, filename)\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Save the requirements to a file inside the folder\n",
        "with open(filepath, \"w\") as f:\n",
        "    f.write(\"streamlit\\nlangchain\\nlangchain-google-genai\\nchromadb\\npython-dotenv\\n\")\n",
        "\n",
        "print(f\"'{filename}' has been saved in the '{folder_name}' folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzfXqBCiZt9I"
      },
      "source": [
        "####**10. Downloading  the entire Project in a Zip file to execute  the Application**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kovlGkUTPhak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b621cff-8927-49a8-93c4-badc77d52fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation' zipped successfully as 'AI_Job_Recommendation.zip'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_886104b6-94eb-4da0-80f3-0fff92ffd55b\", \"AI_Job_Recommendation.zip\", 3826)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the folder to zip\n",
        "folder_name = '/content/drive/MyDrive/DS_Notes/ALMAX_Lite/GenerativeAI/AI_Job_Recommendation'\n",
        "zip_filename = 'AI_Job_Recommendation.zip'\n",
        "\n",
        "# Create a zip archive of the folder\n",
        "try:\n",
        "    shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', folder_name)\n",
        "    print(f\"Folder '{folder_name}' zipped successfully as '{zip_filename}'.\")\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download(zip_filename)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while zipping or downloading the folder: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NGzL7i7Xv0jA",
        "6L11-KksVvPM",
        "4MbJ4HvxWDak",
        "dZ1bHhNwWWjO",
        "Rx4BgxzPWkGD",
        "mbPs90AcXp8j",
        "sh9FTWSFYeFC",
        "a2VyvMCKZhCM",
        "bzfXqBCiZt9I"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}